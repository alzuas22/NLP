{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6EyiPlTVOGsTM3tXBN1fb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**3. Etapa de entrenamiento y testeo de un modelo de análisis\n","de sentimiento**\n","\n","\n","---\n","\n","\n","Importamos las librerías necesarias y cargamos el dataset"],"metadata":{"id":"Xrhl4QenzJeQ"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"cgIQFnUkROCK","executionInfo":{"status":"ok","timestamp":1720384675065,"user_tz":-120,"elapsed":846,"user":{"displayName":"Corona33","userId":"05796263777507625130"}}},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","source":["# Cargamos los datos preprocesados\n","df = pd.read_csv('processed_pet_supplies_reviews.csv')\n","\n","# Mostramos las primeras 5 filas del DataFrame\n","print(df.head())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ycGTRZJzWwk","executionInfo":{"status":"ok","timestamp":1720384928207,"user_tz":-120,"elapsed":538,"user":{"displayName":"Corona33","userId":"05796263777507625130"}},"outputId":"88f30099-18c2-4719-def6-92b9b5450476"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["   overall  verified   reviewTime      reviewerID        asin  \\\n","0      5.0      True  05 27, 2014   AVA8KXONIGNV4  B000FPH1CA   \n","1      5.0      True  02 16, 2014   A60D5HQFOTSOM  B005GSEC3W   \n","2      5.0      True  09 22, 2014  A1BBTVAGN6YIGD  B000WFKTWM   \n","3      1.0     False  08 18, 2017   AS3JM3ZLNJ5DR  B010MVG6ZY   \n","4      2.0      True  09 25, 2016  A38S15PFJIWRYU  B00JZIDCC6   \n","\n","                                               style     reviewerName  \\\n","0  {'Flavor Name:': ' Song Plus', 'Pattern:': ' C...  Victoria Puffer   \n","1  {'Size:': ' 18\" L X 13\" W X 13\" H', 'Color:': ...   DanCooperMedia   \n","2                           {'Size:': ' 16 lb. Bag'}    ShootingStarz   \n","3  {'Size:': ' 100-Count Wipes', 'Color:': ' Unsc...        B. Graham   \n","4  {'Size:': ' 9-Count', 'Flavor Name:': ' Chicke...  Amazon Customer   \n","\n","                                          reviewText  \\\n","0  This product and actualy be eatten by a human....   \n","1  I have a Persian cat. He likes sleeping in thi...   \n","2  I have a (slightly older than) 2 year old, 13 ...   \n","3  I was bringing home a new puppy to live with m...   \n","4  These bones were suggest to buy with a blue ch...   \n","\n","                                             summary  unixReviewTime vote  \\\n","0                                  Fresh fresh fresh      1401148800  NaN   \n","1                                          Very cozy      1392508800  NaN   \n","2  Very Happy With Purina Food & Amazon Auto-Refi...      1411344000    9   \n","3  I used the cloths on them before applying to n...      1503014400  NaN   \n","4              They don't fit suggested dog bone toy      1474761600  NaN   \n","\n","  image                               processed_reviewText  \n","0   NaN  product actualy eatten human one would gives i...  \n","1   NaN  persian cat likes sleeping provided pillow sof...  \n","2   NaN  slightly older two year old thirteen pound mal...  \n","3   NaN  bringing home new puppy live two cats used clo...  \n","4   NaN  bones suggest buy blue chewable bone insert wo...  \n"]}]},{"cell_type":"markdown","source":["Modificaremos el código que hemos creado en la práctica de machine learning para adaptarlo para este ejercicio 3.\n"],"metadata":{"id":"xMCsy9x3zY5q"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","\n","# Eliminamos filas con valores NaN en 'processed_reviewText'\n","df = df.dropna(subset=['processed_reviewText'])\n","\n","# Verificamos el tamaño del dataset después de eliminar NaN\n","dataset_size = len(df)\n","print(f\"Dataset size after dropping NaN: {dataset_size}\")\n","\n","# Usamos una muestra del dataset si el tamaño es mayor a 50,000, de lo contrario usamos todo el dataset\n","sample_size = min(50000, dataset_size)\n","df_sample = df.sample(n=sample_size, random_state=42)\n","\n","# Definimos las características y la variable objetivo\n","X = df_sample['processed_reviewText']\n","y = df_sample['overall'].apply(lambda rating: 1 if rating >= 4 else 0)  # 1 para positivo, 0 para negativo\n","\n","# Dividimos los datos\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Creamos el vectorizador Bag-of-Words\n","vectorizer = CountVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 3)) # ngram_range ya que, como hemos visto, los trigramas también ofrecen información\n","\n","# Ajustamos y transformamos los datos de entrenamiento\n","X_train_bow = vectorizer.fit_transform(X_train)\n","\n","# Transformamos los datos de prueba\n","X_test_bow = vectorizer.transform(X_test)\n","\n","# Entrenamos el modelo de Logistic Regression\n","lr_model = LogisticRegression(random_state=42, max_iter=100)\n","lr_model.fit(X_train_bow, y_train)\n","\n","# Predecimos los valores en el conjunto de prueba\n","y_pred_lr = lr_model.predict(X_test_bow)\n","\n","# Entrenamos el modelo de Random Forest\n","rf_model = RandomForestClassifier(random_state=42, n_estimators=50, max_depth=10)\n","rf_model.fit(X_train_bow, y_train)\n","\n","# Predecimos los valores en el conjunto de prueba\n","y_pred_rf = rf_model.predict(X_test_bow)\n","\n","# Evaluamos el modelo de Logistic Regression\n","print(\"Logistic Regression Model:\")\n","print(classification_report(y_test, y_pred_lr))\n","\n","# Evaluamos el modelo de Random Forest\n","print(\"Random Forest Model:\")\n","print(classification_report(y_test, y_pred_rf))\n","\n","# Comparamos las métricas\n","lr_report = classification_report(y_test, y_pred_lr, output_dict=True)\n","rf_report = classification_report(y_test, y_pred_rf, output_dict=True)\n","\n","# Mostramos las métricas de precisión, recall y f1-score para ambos modelos\n","print(f\"Logistic Regression - Precision: {lr_report['1']['precision']}, Recall: {lr_report['1']['recall']}, F1-score: {lr_report['1']['f1-score']}\")\n","print(f\"Random Forest - Precision: {rf_report['1']['precision']}, Recall: {rf_report['1']['recall']}, F1-score: {rf_report['1']['f1-score']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDK0QuUmzYFK","executionInfo":{"status":"ok","timestamp":1720385436658,"user_tz":-120,"elapsed":10811,"user":{"displayName":"Corona33","userId":"05796263777507625130"}},"outputId":"e385c409-faae-4b5b-e55d-be0d3ded40bf"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size after dropping NaN: 49940\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["Logistic Regression Model:\n","              precision    recall  f1-score   support\n","\n","           0       0.70      0.44      0.54      2017\n","           1       0.87      0.95      0.91      7971\n","\n","    accuracy                           0.85      9988\n","   macro avg       0.79      0.69      0.72      9988\n","weighted avg       0.84      0.85      0.83      9988\n","\n","Random Forest Model:\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.02      0.05      2017\n","           1       0.80      1.00      0.89      7971\n","\n","    accuracy                           0.80      9988\n","   macro avg       0.87      0.51      0.47      9988\n","weighted avg       0.83      0.80      0.72      9988\n","\n","Logistic Regression - Precision: 0.8697094486387554, Recall: 0.9538326433320787, F1-score: 0.9098306707353556\n","Random Forest - Precision: 0.8020130850528435, Recall: 0.9996236356793377, F1-score: 0.8899810119513012\n"]}]},{"cell_type":"markdown","source":["Vamos a intentar mejorar estos primeros parámetros para ver que sucede por ahora tenemos en cuenta que el mejor de todos ha sido la logística"],"metadata":{"id":"rYN9ZHqLXnc6"}},{"cell_type":"code","source":["# Importar librerías necesarias\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","\n","# Cargamos los datos preprocesados\n","df = pd.read_csv('processed_pet_supplies_reviews.csv')\n","\n","# Eliminamos filas con valores NaN en 'processed_reviewText'\n","df = df.dropna(subset=['processed_reviewText'])\n","\n","# Verificamos el tamaño del dataset después de eliminar NaN\n","dataset_size = len(df)\n","print(f\"Dataset size after dropping NaN: {dataset_size}\")\n","\n","# Usamos una muestra del dataset si el tamaño es mayor a 50,000, de lo contrario usamos todo el dataset\n","sample_size = min(50000, dataset_size)\n","df_sample = df.sample(n=sample_size, random_state=42)\n","\n","# Definimos las características y la variable objetivo\n","X = df_sample['processed_reviewText']\n","y = df_sample['overall'].apply(lambda rating: 1 if rating >= 4 else 0)  # 1 para positivo, 0 para negativo\n","\n","# Dividimos los datos\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Creamos el vectorizador Bag-of-Words\n","vectorizer = CountVectorizer(max_features=1000, stop_words='english', ngram_range=(1, 3)) # ngram_range ya que, como hemos visto, los trigramas también ofrecen información\n","\n","# Ajustamos y transformamos los datos de entrenamiento\n","X_train_bow = vectorizer.fit_transform(X_train)\n","\n","# Transformamos los datos de prueba\n","X_test_bow = vectorizer.transform(X_test)\n","\n","# Definimos los parámetros de la búsqueda para Logistic Regression\n","param_grid_lr = {\n","    'penalty': ['l2'],  # Limitar a 'l2' para reducir el espacio de búsqueda\n","    'C': [0.01, 0.05, 0.25, 0.5, 1, 10, 100, 1000, 10000],\n","    'solver': ['liblinear']\n","}\n","\n","# Creamos el modelo de Logistic Regression\n","lr = LogisticRegression(random_state=42)\n","\n","# Configuramos la búsqueda con GridSearchCV para Logistic Regression\n","grid_search_lr = GridSearchCV(estimator=lr, param_grid=param_grid_lr, cv=3, scoring='f1', n_jobs=-1)\n","\n","# Ajustamos la búsqueda a los datos de entrenamiento\n","grid_search_lr.fit(X_train_bow, y_train)\n","\n","# Mejor modelo encontrado por la búsqueda para Logistic Regression\n","best_lr_model = grid_search_lr.best_estimator_\n","\n","# Predecimos los valores en el conjunto de prueba con el mejor modelo de Logistic Regression\n","y_pred_best_lr = best_lr_model.predict(X_test_bow)\n","\n","# Evaluamos el mejor modelo de Logistic Regression\n","print(f\"Best Logistic Regression Model With C = {grid_search_lr.best_params_['C']}:\")\n","print(classification_report(y_test, y_pred_best_lr))\n","\n","# Definimos los parámetros de la búsqueda para Random Forest\n","param_grid_rf = {\n","    'n_estimators': [50, 100],\n","    'max_depth': [10, 20],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2]\n","}\n","\n","# Creamos el modelo de Random Forest\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Configuramos la búsqueda con GridSearchCV para Random Forest\n","grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=3, scoring='f1', n_jobs=-1)\n","\n","# Ajustamos la búsqueda a los datos de entrenamiento\n","grid_search_rf.fit(X_train_bow, y_train)\n","\n","# Mejor modelo encontrado por la búsqueda para Random Forest\n","best_rf_model = grid_search_rf.best_estimator_\n","\n","# Predecimos los valores en el conjunto de prueba con el mejor modelo de Random Forest\n","y_pred_best_rf = best_rf_model.predict(X_test_bow)\n","\n","# Evaluamos el mejor modelo de Random Forest\n","print(\"Best Random Forest Model:\")\n","print(f\"Best parameters: {grid_search_rf.best_params_}\")\n","print(classification_report(y_test, y_pred_best_rf))\n","\n","# Comparamos las métricas de los mejores modelos\n","best_lr_report = classification_report(y_test, y_pred_best_lr, output_dict=True)\n","best_rf_report = classification_report(y_test, y_pred_best_rf, output_dict=True)\n","\n","# Mostramos las métricas de precisión, recall y f1-score para ambos modelos\n","print(f\"Best Logistic Regression - Precision: {best_lr_report['1']['precision']}, Recall: {best_lr_report['1']['recall']}, F1-score: {best_lr_report['1']['f1-score']}\")\n","print(f\"Best Random Forest - Precision: {best_rf_report['1']['precision']}, Recall: {best_rf_report['1']['recall']}, F1-score: {best_rf_report['1']['f1-score']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KuurEDfVXys5","executionInfo":{"status":"ok","timestamp":1720386001497,"user_tz":-120,"elapsed":69507,"user":{"displayName":"Corona33","userId":"05796263777507625130"}},"outputId":"41eabd49-ff9f-49e7-bda4-025b281afb1f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size after dropping NaN: 49940\n","Best Logistic Regression Model With C = 0.25:\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.41      0.52      2017\n","           1       0.87      0.96      0.91      7971\n","\n","    accuracy                           0.85      9988\n","   macro avg       0.79      0.68      0.72      9988\n","weighted avg       0.83      0.85      0.83      9988\n","\n","Best Random Forest Model:\n","Best parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 50}\n","              precision    recall  f1-score   support\n","\n","           0       0.92      0.06      0.10      2017\n","           1       0.81      1.00      0.89      7971\n","\n","    accuracy                           0.81      9988\n","   macro avg       0.86      0.53      0.50      9988\n","weighted avg       0.83      0.81      0.73      9988\n","\n","Best Logistic Regression - Precision: 0.8654195011337869, Recall: 0.9575962865387028, F1-score: 0.9091775355845393\n","Best Random Forest - Precision: 0.8068308503091112, Recall: 0.9987454522644587, F1-score: 0.8925888552528309\n"]}]},{"cell_type":"markdown","source":["Seguimos viendo que la logística es la que mejor funciona  y que no hemos conseguido una mejora significativa al coger los parámetros óptimos. Esto se puede deber a que no estamos realizando una búsqueda intensiva de los mismos ya que el tiempo de carga de los apartados puede crecer de forma exponencial."],"metadata":{"id":"fSzsrvpGao_k"}},{"cell_type":"markdown","source":["Vamos a realizar predicciones con el modelo para visualizar la potencia de estos modelos que  hemos creado."],"metadata":{"id":"_-Ryyv1FDLm9"}},{"cell_type":"code","source":["import random\n","\n","def predict_review_sentiment(review_index, model):\n","    print('Actual sentiment: {}'.format(1 if df.iloc[review_index]['overall'] >= 4 else 0))\n","    r = df.iloc[review_index]['processed_reviewText']\n","    print('Prediction: {}'.format(best_lr_model.predict(vectorizer.transform([r]))))\n","\n","for i in random.sample(range(0, len(df)), 5):\n","    print('\\nReview no. {}'.format(i))\n","    predict_review_sentiment(i, best_lr_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eJhGPwcfDFPF","executionInfo":{"status":"ok","timestamp":1720203180533,"user_tz":-120,"elapsed":263,"user":{"displayName":"Corona33","userId":"05796263777507625130"}},"outputId":"b495ebe7-d3b8-4dc1-8257-498bd3894cfc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Review no. 38319\n","Actual sentiment: 1\n","Prediction: [1]\n","\n","Review no. 17274\n","Actual sentiment: 1\n","Prediction: [1]\n","\n","Review no. 46942\n","Actual sentiment: 0\n","Prediction: [1]\n","\n","Review no. 42676\n","Actual sentiment: 1\n","Prediction: [1]\n","\n","Review no. 32359\n","Actual sentiment: 1\n","Prediction: [1]\n"]}]},{"cell_type":"code","source":["def predict_review_sentiment(review_index, model):\n","    print('Actual sentiment: {}'.format(1 if df.iloc[review_index]['overall'] >= 4 else 0))\n","    r = df.iloc[review_index]['processed_reviewText']\n","    print('Prediction: {}'.format(best_rf_model.predict(vectorizer.transform([r]))))\n","\n","for i in random.sample(range(0, len(df)), 5):\n","    print('\\nReview no. {}'.format(i))\n","    predict_review_sentiment(i, best_rf_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xWGhNsfHGbXA","executionInfo":{"status":"ok","timestamp":1720203222577,"user_tz":-120,"elapsed":307,"user":{"displayName":"Corona33","userId":"05796263777507625130"}},"outputId":"edd9fe70-b68c-4ad2-cb66-d6fbdac46367"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Review no. 5362\n","Actual sentiment: 0\n","Prediction: [1]\n","\n","Review no. 3726\n","Actual sentiment: 1\n","Prediction: [1]\n","\n","Review no. 5954\n","Actual sentiment: 1\n","Prediction: [1]\n","\n","Review no. 43661\n","Actual sentiment: 1\n","Prediction: [1]\n","\n","Review no. 44486\n","Actual sentiment: 0\n","Prediction: [1]\n"]}]},{"cell_type":"markdown","source":["Probemos ahora a usar el TfidfVectorizer en vez del CountVectorizer a ver si conseguimos mejores resultados."],"metadata":{"id":"_FrUPbSMANxY"}},{"cell_type":"code","source":["# Importar librerías necesarias\n","import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","\n","# Cargamos los datos preprocesados\n","df = pd.read_csv('processed_pet_supplies_reviews.csv')\n","\n","# Eliminamos filas con valores NaN en 'processed_reviewText'\n","df = df.dropna(subset=['processed_reviewText'])\n","\n","# Verificamos el tamaño del dataset después de eliminar NaN\n","dataset_size = len(df)\n","print(f\"Dataset size after dropping NaN: {dataset_size}\")\n","\n","# Usamos una muestra del dataset si el tamaño es mayor a 50,000, de lo contrario usamos todo el dataset\n","sample_size = min(50000, dataset_size)\n","df_sample = df.sample(n=sample_size, random_state=42)\n","\n","# Definimos las características y la variable objetivo\n","X = df_sample['processed_reviewText']\n","y = df_sample['overall'].apply(lambda rating: 1 if rating >= 4 else 0)  # 1 para positivo, 0 para negativo\n","\n","# Dividimos los datos\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","\n","# Creamos el vectorizador Bag-of-Words\n","vectorizer = TfidfVectorizer(\n","    max_df=0.95,\n","    min_df=3,\n","    max_features=2500,\n","    strip_accents='ascii',\n","    ngram_range=(1, 3)\n",")\n","\n","# Ajustamos y transformamos los datos de entrenamiento\n","X_train_bow = vectorizer.fit_transform(X_train)\n","\n","# Transformamos los datos de prueba\n","X_test_bow = vectorizer.transform(X_test)\n","\n","# Definimos los parámetros de la búsqueda para Logistic Regression\n","param_grid_lr = {\n","    'penalty': ['l2'],  # Limitar a 'l2' para reducir el espacio de búsqueda\n","    'C': [0.01, 0.05, 0.25, 0.5, 1, 10, 100, 1000, 10000],\n","    'solver': ['liblinear']\n","}\n","\n","# Creamos el modelo de Logistic Regression\n","lr = LogisticRegression(random_state=42)\n","\n","# Configuramos la búsqueda con GridSearchCV para Logistic Regression\n","grid_search_lr = GridSearchCV(estimator=lr, param_grid=param_grid_lr, cv=3, scoring='f1', n_jobs=-1)\n","\n","# Ajustamos la búsqueda a los datos de entrenamiento\n","grid_search_lr.fit(X_train_bow, y_train)\n","\n","# Mejor modelo encontrado por la búsqueda para Logistic Regression\n","best_lr_model = grid_search_lr.best_estimator_\n","\n","# Predecimos los valores en el conjunto de prueba con el mejor modelo de Logistic Regression\n","y_pred_best_lr = best_lr_model.predict(X_test_bow)\n","\n","# Evaluamos el mejor modelo de Logistic Regression\n","print(f\"Best Logistic Regression Model With C = {grid_search_lr.best_params_['C']}:\")\n","print(classification_report(y_test, y_pred_best_lr))\n","\n","# Definimos los parámetros de la búsqueda para Random Forest\n","param_grid_rf = {\n","    'n_estimators': [50, 100],\n","    'max_depth': [10, 20],\n","    'min_samples_split': [2, 5],\n","    'min_samples_leaf': [1, 2]\n","}\n","\n","# Creamos el modelo de Random Forest\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Configuramos la búsqueda con GridSearchCV para Random Forest\n","grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, cv=3, scoring='f1', n_jobs=-1)\n","\n","# Ajustamos la búsqueda a los datos de entrenamiento\n","grid_search_rf.fit(X_train_bow, y_train)\n","\n","# Mejor modelo encontrado por la búsqueda para Random Forest\n","best_rf_model = grid_search_rf.best_estimator_\n","\n","# Predecimos los valores en el conjunto de prueba con el mejor modelo de Random Forest\n","y_pred_best_rf = best_rf_model.predict(X_test_bow)\n","\n","# Evaluamos el mejor modelo de Random Forest\n","print(\"Best Random Forest Model:\")\n","print(f\"Best parameters: {grid_search_rf.best_params_}\")\n","print(classification_report(y_test, y_pred_best_rf))\n","\n","# Comparamos las métricas de los mejores modelos\n","best_lr_report = classification_report(y_test, y_pred_best_lr, output_dict=True)\n","best_rf_report = classification_report(y_test, y_pred_best_rf, output_dict=True)\n","\n","# Mostramos las métricas de precisión, recall y f1-score para ambos modelos\n","print(f\"Best Logistic Regression - Precision: {best_lr_report['1']['precision']}, Recall: {best_lr_report['1']['recall']}, F1-score: {best_lr_report['1']['f1-score']}\")\n","print(f\"Best Random Forest - Precision: {best_rf_report['1']['precision']}, Recall: {best_rf_report['1']['recall']}, F1-score: {best_rf_report['1']['f1-score']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ANl9FQuAfRK","executionInfo":{"status":"ok","timestamp":1720386349592,"user_tz":-120,"elapsed":75563,"user":{"displayName":"Corona33","userId":"05796263777507625130"}},"outputId":"538b3adc-926e-4603-e6ee-547db445c4f1"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset size after dropping NaN: 49940\n","Best Logistic Regression Model With C = 1:\n","              precision    recall  f1-score   support\n","\n","           0       0.77      0.50      0.61      2017\n","           1       0.88      0.96      0.92      7971\n","\n","    accuracy                           0.87      9988\n","   macro avg       0.83      0.73      0.77      9988\n","weighted avg       0.86      0.87      0.86      9988\n","\n","Best Random Forest Model:\n","Best parameters: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n","              precision    recall  f1-score   support\n","\n","           0       0.95      0.05      0.10      2017\n","           1       0.81      1.00      0.89      7971\n","\n","    accuracy                           0.81      9988\n","   macro avg       0.88      0.52      0.49      9988\n","weighted avg       0.84      0.81      0.73      9988\n","\n","Best Logistic Regression - Precision: 0.8847262247838616, Recall: 0.9628653870279764, F1-score: 0.9221434578877808\n","Best Random Forest - Precision: 0.8061937050905779, Recall: 0.9993727261322294, F1-score: 0.892449025319292\n"]}]}]}